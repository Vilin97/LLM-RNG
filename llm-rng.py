# Curio: when asked to guess a number between 1 and 50, most LLMs will guess 27.
# I want to explore random number generation in LLMs.
# Questions:
# 1. What is the distribution of numbers generated by LLMs using prompt "Guess a random number between 1 and 50"?
# 2. How does the distribution change with different prompts? E.g. "Roll a 50-sided die", or "Generate a completely random number between 1 and 50".
# 3. How does the distribution change with different LLMs? Do bigger LLMs have more or less skewed distributions of random numbers? Which LLM is the most random?
# 4. How does the temperature setting affect the distribution?
# 5. Do reasoning models generate more random numbers than non-reasoning models?
# 6. Are integers easier or harder than fractions? E.g. instead of numbers between 1 and 50, numbers between 0. and 0.5.
# 7. Is it harder to sample from non-uniform distributions? E.g. standard normal or mean-one-exponential distributions?
# 8. Do LLMs have an easier time sampling from different ranges? E.g. 1-10 or 1000-2000 or even 100_000 to 1_000_000?

# Models to test: 
# good at coding: gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, 
# reasoning: o3, o4-mini, 
# general: gpt-4o, gpt-4o-mini, gpt-4.5-preview (this one is the largest model)

# Tips: 
# 1. First, design a very small experiment, then scale it up
# 2. Track and report distribution of invalid responses as well, like if the model gives several numbers or a number outside the range, etc. Maybe smaller models are more likely to do that.
# 3. Save the data from time to time, not just at the end, so that I can stop the experiment and still have some results.
# 4. Keep the code concise, don't over-engineer it.
# 5. Keep track of the number of tokens used. Print this number from time to time.
# 6. Don't run a "grid search" over all configurations. For each question, run a single experiment. E.g. if the question is about changing the prompt, pick a single model, single temperature, etc. 

import pandas as pd
from tqdm import trange, tqdm
from openai import OpenAI
import json
import time
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np
import glob
import os

client = OpenAI()
TOKENS_USED = 0

# Configuration for data loading vs generation
USE_EXISTING_DATA = True  # Set to True to load existing data instead of generating new
DATA_DIRECTORY = "."  # Directory to look for existing data files

def find_latest_experiment_file(experiment_num, file_type="results"):
    """Find the most recent file for a given experiment"""
    if file_type == "results":
        pattern = f"exp_{experiment_num}_*_results_*.csv"
    else:
        pattern = f"exp_{experiment_num}_*_{file_type}_*.csv"
    
    files = glob.glob(os.path.join(DATA_DIRECTORY, pattern))
    if not files:
        return None
    
    # Sort by modification time, return most recent
    files.sort(key=os.path.getmtime, reverse=True)
    return files[0]

def load_experiment_data(experiment_num, description=None):
    """Load existing experiment data from CSV file"""
    filename = find_latest_experiment_file(experiment_num)
    
    if filename is None:
        raise FileNotFoundError(f"No existing data found for experiment {experiment_num}")
    
    print(f"üìÅ Loading existing data from: {filename}")
    df = pd.read_csv(filename)
    
    # Print basic info about loaded data
    print(f"Loaded {len(df)} samples from {len(df['model'].unique())} models")
    print(f"Validity breakdown: {dict(df['validity'].value_counts())}")
    
    return df, filename

def get_random_number(model, prompt, temperature=0.7):
    """Get a random number from an LLM with the given prompt"""
    global TOKENS_USED
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a random number generator. Your job is to output a single number and nothing else."},
                {"role": "user", "content": prompt}
            ],
            temperature=temperature,
            max_tokens=10
        )
        TOKENS_USED += response.usage.total_tokens
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"ERROR: {str(e)}"

def parse_number(response, min_val=1, max_val=50):
    """Parse the response and extract a valid number, track invalid responses"""
    response = response.strip()
    
    # Handle error responses
    if response.startswith("ERROR:"):
        return None, "api_error"
    
    # Try to extract a number from the response
    import re
    numbers = re.findall(r'\d+', response)
    
    if not numbers:
        return None, "no_number"
    
    # If multiple numbers, take the first one
    try:
        number = int(numbers[0])
        if min_val <= number <= max_val:
            return number, "valid"
        else:
            return number, "out_of_range"
    except ValueError:
        return None, "parse_error"

def run_experiment(models, prompts, n_samples=10, temperature=0.7):
    """Run a small experiment with given models and prompts"""
    results = []
    
    print(f"Starting experiment with {len(models)} models, {len(prompts)} prompts, {n_samples} samples each")
    print(f"Temperature: {temperature}")
    
    for model in models:
        for prompt_name, prompt_text in prompts.items():
            print(f"\nTesting {model} with prompt '{prompt_name}'...")
            
            for i in trange(n_samples, desc=f"{model}-{prompt_name}"):
                response = get_random_number(model, prompt_text, temperature)
                number, validity = parse_number(response)
                
                results.append({
                    'timestamp': datetime.now().isoformat(),
                    'model': model,
                    'prompt_name': prompt_name,
                    'prompt_text': prompt_text,
                    'temperature': temperature,
                    'raw_response': response,
                    'parsed_number': number,
                    'validity': validity,
                    'sample_id': i
                })
                
                # Small delay to be nice to the API
                time.sleep(0.1)
    
    return pd.DataFrame(results)

def save_results(df, filename=None, experiment_name="general"):
    """Save results to CSV with timestamp and experiment info"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"exp_{experiment_name}_results_{timestamp}.csv"
    
    df.to_csv(filename, index=False)
    print(f"Results saved to {filename}")
    return filename

def analyze_results(df):
    """Quick analysis of the results"""
    print("\n=== EXPERIMENT RESULTS ===")
    print(f"Total tokens used: {TOKENS_USED}")
    print(f"Total samples: {len(df)}")
    
    # Validity breakdown
    print("\nValidity breakdown:")
    print(df['validity'].value_counts())
    
    # Valid numbers only
    valid_df = df[df['validity'] == 'valid'].copy()
    if len(valid_df) > 0:
        print(f"\nValid responses: {len(valid_df)}/{len(df)} ({len(valid_df)/len(df)*100:.1f}%)")
        
        # Distribution by model and prompt
        for model in df['model'].unique():
            for prompt_name in df['prompt_name'].unique():
                subset = valid_df[(valid_df['model'] == model) & (valid_df['prompt_name'] == prompt_name)]
                if len(subset) > 0:
                    numbers = subset['parsed_number'].tolist()
                    print(f"\n{model} - {prompt_name}: {numbers}")
                    print(f"  Mean: {subset['parsed_number'].mean():.2f}, Std: {subset['parsed_number'].std():.2f}")

def plot_histograms(df, save_plot=True, experiment_name="general"):
    """Plot histograms for each model showing the distribution of generated numbers"""
    valid_df = df[df['validity'] == 'valid'].copy()
    
    if len(valid_df) == 0:
        print("No valid data to plot!")
        return plt.figure()
    
    models = valid_df['model'].unique()
    n_models = len(models)
    
    # Create subplots - 2 columns for better layout
    n_cols = 2
    n_rows = (n_models + 1) // 2
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4*n_rows))
    
    # Fix axes handling to always have a list
    if n_models == 1:
        if n_rows == 1 and n_cols == 1:
            axes = [axes]  # Single subplot case
        else:
            axes = axes.flatten()
    elif n_rows == 1:
        axes = axes if n_cols > 1 else [axes]  # Single row case
    else:
        axes = axes.flatten()  # Multiple rows case
    
    # Plot histogram for each model
    for i, model in enumerate(models):
        model_data = valid_df[valid_df['model'] == model]['parsed_number']
        
        ax = axes[i]
        ax.hist(model_data, bins=range(1, 52), alpha=0.7, edgecolor='black')
        ax.set_title(f'{model}\n(n={len(model_data)}, mean={model_data.mean():.1f}, std={model_data.std():.1f})')
        ax.set_xlabel('Number Generated')
        ax.set_ylabel('Frequency')
        ax.set_xlim(0, 51)
        ax.grid(True, alpha=0.3)
        
        # Add vertical line at mean
        ax.axvline(model_data.mean(), color='red', linestyle='--', alpha=0.7, label=f'Mean: {model_data.mean():.1f}')
        ax.legend()
    
    # Hide unused subplots
    for i in range(n_models, len(axes)):
        axes[i].set_visible(False)
    
    plt.tight_layout()
    
    if save_plot:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"exp_{experiment_name}_histograms_{timestamp}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"Histogram saved to {filename}")
    
    # Don't show plot interactively in automated experiments
    plt.close()
    return fig

def plot_prompt_comparison(df):
    """Plot histograms comparing different prompts (for experiment 2)"""
    valid_df = df[df['validity'] == 'valid'].copy()
    
    if len(valid_df) == 0:
        print("No valid data to plot!")
        return plt.figure()
    
    prompts = valid_df['prompt_name'].unique()
    n_prompts = len(prompts)
    
    # Create subplots - 2 columns for better layout
    n_cols = 2
    n_rows = (n_prompts + 1) // 2
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4*n_rows))
    
    # Fix axes handling
    if n_prompts == 1:
        axes = [axes] if n_rows == 1 and n_cols == 1 else axes.flatten()
    elif n_rows == 1:
        axes = axes if n_cols > 1 else [axes]
    else:
        axes = axes.flatten()
    
    # Plot histogram for each prompt
    for i, prompt in enumerate(prompts):
        prompt_data = valid_df[valid_df['prompt_name'] == prompt]['parsed_number']
        
        ax = axes[i]
        ax.hist(prompt_data, bins=range(1, 52), alpha=0.7, edgecolor='black')
        ax.set_title(f'{prompt}\n(n={len(prompt_data)}, mean={prompt_data.mean():.1f}, std={prompt_data.std():.1f})')
        ax.set_xlabel('Number Generated')
        ax.set_ylabel('Frequency')
        ax.set_xlim(0, 51)
        ax.grid(True, alpha=0.3)
        
        # Add vertical line at mean
        ax.axvline(prompt_data.mean(), color='red', linestyle='--', alpha=0.7, label=f'Mean: {prompt_data.mean():.1f}')
        ax.legend()
    
    # Hide unused subplots
    for i in range(n_prompts, len(axes)):
        axes[i].set_visible(False)
    
    plt.tight_layout()
    return fig

def plot_temperature_effects(df):
    """Plot histograms comparing different temperatures (for experiment 4)"""
    valid_df = df[df['validity'] == 'valid'].copy()
    
    if len(valid_df) == 0:
        print("No valid data to plot!")
        return plt.figure()
    
    temperatures = sorted(valid_df['temperature_label'].unique())
    n_temps = len(temperatures)
    
    # Create subplots - 3 columns for better layout
    n_cols = 3
    n_rows = (n_temps + 2) // 3
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))
    
    # Fix axes handling
    if n_temps == 1:
        axes = [axes] if n_rows == 1 and n_cols == 1 else axes.flatten()
    elif n_rows == 1:
        axes = axes if n_cols > 1 else [axes]
    else:
        axes = axes.flatten()
    
    # Plot histogram for each temperature
    for i, temp in enumerate(temperatures):
        temp_data = valid_df[valid_df['temperature_label'] == temp]['parsed_number']
        
        ax = axes[i]
        ax.hist(temp_data, bins=range(1, 52), alpha=0.7, edgecolor='black')
        ax.set_title(f'Temperature {temp}\n(n={len(temp_data)}, mean={temp_data.mean():.1f}, std={temp_data.std():.1f})')
        ax.set_xlabel('Number Generated')
        ax.set_ylabel('Frequency')
        ax.set_xlim(0, 51)
        ax.grid(True, alpha=0.3)
        
        # Add vertical line at mean
        ax.axvline(temp_data.mean(), color='red', linestyle='--', alpha=0.7, label=f'Mean: {temp_data.mean():.1f}')
        ax.legend()
    
    # Hide unused subplots
    for i in range(n_temps, len(axes)):
        axes[i].set_visible(False)
    
    plt.tight_layout()
    return fig

def plot_model_type_comparison(df):
    """Plot histograms comparing reasoning vs normal models (for experiment 5)"""
    valid_df = df[df['validity'] == 'valid'].copy()
    
    if len(valid_df) == 0:
        print("No valid data to plot!")
        return plt.figure()
    
    # First plot by model type
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    for i, model_type in enumerate(['reasoning', 'normal']):
        type_data = valid_df[valid_df['model_type'] == model_type]['parsed_number']
        
        if len(type_data) > 0:
            axes[i].hist(type_data, bins=range(1, 52), alpha=0.7, edgecolor='black')
            axes[i].set_title(f'{model_type.title()} Models\n(n={len(type_data)}, mean={type_data.mean():.1f}, std={type_data.std():.1f})')
            axes[i].set_xlabel('Number Generated')
            axes[i].set_ylabel('Frequency')
            axes[i].set_xlim(0, 51)
            axes[i].grid(True, alpha=0.3)
            axes[i].axvline(type_data.mean(), color='red', linestyle='--', alpha=0.7, label=f'Mean: {type_data.mean():.1f}')
            axes[i].legend()
        else:
            axes[i].set_title(f'{model_type.title()} Models\n(No data)')
    
    plt.tight_layout()
    return fig

def plot_number_type_comparison(df):
    """Plot histograms comparing integers vs fractions (for experiment 6)"""
    valid_df = df[df['validity'] == 'valid'].copy()
    
    if len(valid_df) == 0:
        print("No valid data to plot!")
        return plt.figure()
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # Integer data
    int_data = valid_df[valid_df['number_type'] == 'integer']['parsed_number']
    if len(int_data) > 0:
        axes[0].hist(int_data, bins=range(1, 52), alpha=0.7, edgecolor='black')
        axes[0].set_title(f'Integer Sampling\n(n={len(int_data)}, mean={int_data.mean():.1f}, std={int_data.std():.1f})')
        axes[0].set_xlabel('Number Generated')
        axes[0].set_ylabel('Frequency')
        axes[0].set_xlim(0, 51)
        axes[0].grid(True, alpha=0.3)
        axes[0].axvline(int_data.mean(), color='red', linestyle='--', alpha=0.7, label=f'Mean: {int_data.mean():.1f}')
        axes[0].legend()
    else:
        axes[0].set_title('Integer Sampling\n(No data)')
    
    # Fraction data
    frac_data = valid_df[valid_df['number_type'] == 'fraction']['parsed_number']
    if len(frac_data) > 0:
        axes[1].hist(frac_data, bins=20, alpha=0.7, edgecolor='black')
        axes[1].set_title(f'Fraction Sampling\n(n={len(frac_data)}, mean={frac_data.mean():.3f}, std={frac_data.std():.3f})')
        axes[1].set_xlabel('Number Generated')
        axes[1].set_ylabel('Frequency')
        axes[1].set_xlim(0, 0.5)
        axes[1].grid(True, alpha=0.3)
        axes[1].axvline(frac_data.mean(), color='red', linestyle='--', alpha=0.7, label=f'Mean: {frac_data.mean():.3f}')
        axes[1].legend()
    else:
        axes[1].set_title('Fraction Sampling\n(No data)')
    
    plt.tight_layout()
    return fig

def plot_distribution_comparison(df):
    """Plot histograms comparing different distributions (for experiment 7)"""
    valid_df = df[df['validity'] == 'valid'].copy()
    
    if len(valid_df) == 0:
        print("No valid data to plot!")
        return plt.figure()
    
    distributions = valid_df['prompt_name'].unique()
    n_dists = len(distributions)
    
    fig, axes = plt.subplots(1, n_dists, figsize=(5*n_dists, 5))
    
    if n_dists == 1:
        axes = [axes]
    
    for i, dist in enumerate(distributions):
        dist_data = valid_df[valid_df['prompt_name'] == dist]['parsed_number']
        
        if len(dist_data) > 0:
            # Use different bin strategies for different distributions
            if dist == 'uniform':
                bins = range(1, 52)
                xlim = (0, 51)
            else:
                # For normal and exponential, use automatic binning
                bins = 20
                data_range = dist_data.max() - dist_data.min()
                xlim = (dist_data.min() - 0.1*data_range, dist_data.max() + 0.1*data_range)
            
            axes[i].hist(dist_data, bins=bins, alpha=0.7, edgecolor='black')
            axes[i].set_title(f'{dist.title()} Distribution\n(n={len(dist_data)}, mean={dist_data.mean():.2f}, std={dist_data.std():.2f})')
            axes[i].set_xlabel('Number Generated')
            axes[i].set_ylabel('Frequency')
            axes[i].set_xlim(xlim)
            axes[i].grid(True, alpha=0.3)
            axes[i].axvline(dist_data.mean(), color='red', linestyle='--', alpha=0.7, label=f'Mean: {dist_data.mean():.2f}')
            axes[i].legend()
        else:
            axes[i].set_title(f'{dist.title()} Distribution\n(No data)')
    
    plt.tight_layout()
    return fig

def plot_range_comparison(df):
    """Plot histograms comparing different ranges (for experiment 8)"""
    valid_df = df[df['validity'] == 'valid'].copy()
    
    if len(valid_df) == 0:
        print("No valid data to plot!")
        return plt.figure()
    
    ranges = ['small', 'medium', 'large', 'huge']
    range_info = {
        'small': (1, 10),
        'medium': (1, 50),
        'large': (1000, 2000),
        'huge': (100000, 1000000)
    }
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.flatten()
    
    for i, range_type in enumerate(ranges):
        range_data = valid_df[valid_df['range_type'] == range_type]['parsed_number']
        
        if len(range_data) > 0:
            min_val, max_val = range_info[range_type]
            
            # Use appropriate binning for each range
            if range_type == 'small':
                bins = range(1, 12)
            elif range_type == 'medium':
                bins = range(1, 52)
            else:
                bins = 20  # Auto binning for large ranges
            
            axes[i].hist(range_data, bins=bins, alpha=0.7, edgecolor='black')
            axes[i].set_title(f'{range_type.title()} Range ({min_val}-{max_val})\n(n={len(range_data)}, mean={range_data.mean():.0f})')
            axes[i].set_xlabel('Number Generated')
            axes[i].set_ylabel('Frequency')
            axes[i].grid(True, alpha=0.3)
            axes[i].axvline(range_data.mean(), color='red', linestyle='--', alpha=0.7, label=f'Mean: {range_data.mean():.0f}')
            axes[i].legend()
        else:
            axes[i].set_title(f'{range_type.title()} Range\n(No data)')
    
    plt.tight_layout()
    return fig

# EXPERIMENT FRAMEWORK FUNCTIONS

def run_question_experiment(question_num, description, models, prompts, n_samples=20, temperature=0.7, custom_parser=None):
    """Run an experiment for a specific research question"""
    global TOKENS_USED, USE_EXISTING_DATA
    
    print(f"\n{'='*60}")
    print(f"EXPERIMENT {question_num}: {description}")
    print(f"{'='*60}")
    
    # Check if we should load existing data instead
    if USE_EXISTING_DATA:
        try:
            df, filename = load_experiment_data(question_num, description)
            print(f"‚úÖ Using existing data for experiment {question_num}")
            return df, filename
        except FileNotFoundError as e:
            print(f"‚ö†Ô∏è  {e}")
            print("Falling back to generating new data...")
    
    # Reset token counter for this experiment
    tokens_start = TOKENS_USED
    
    results = []
    
    # Calculate total iterations for progress bar
    total_iterations = len(models) * len(prompts) * n_samples
    
    with tqdm(total=total_iterations, desc=f"Experiment {question_num}") as pbar:
        for model in models:
            for prompt_name, prompt_text in prompts.items():
                print(f"\nTesting {model} with prompt '{prompt_name}'...")
                
                for i in range(n_samples):
                    response = get_random_number(model, prompt_text, temperature)
                    
                    # Use custom parser if provided, otherwise default
                    if custom_parser:
                        number, validity = custom_parser(response)
                    else:
                        number, validity = parse_number(response)
                    
                    results.append({
                        'question': question_num,
                        'timestamp': datetime.now().isoformat(),
                        'model': model,
                        'prompt_name': prompt_name,
                        'prompt_text': prompt_text,
                        'temperature': temperature,
                        'raw_response': response,
                        'parsed_number': number,
                        'validity': validity,
                        'sample_id': i
                    })
                    
                    pbar.update(1)
                    time.sleep(0.1)
    
    df = pd.DataFrame(results)
    
    # Save results with clear experiment naming
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"exp_{question_num}_{description.lower().replace(' ', '_')}_results_{timestamp}.csv"
    df.to_csv(filename, index=False)
    
    # Print token usage for this experiment
    tokens_used = TOKENS_USED - tokens_start
    print(f"\nüìä EXPERIMENT {question_num} COMPLETED")
    print(f"Tokens used for this experiment: {tokens_used:,}")
    print(f"Total tokens used so far: {TOKENS_USED:,}")
    print(f"Results saved to: {filename}")
    
    return df, filename

def plot_question_results(df, question_num, description, custom_plot_func=None):
    """Plot and save results for a specific question"""
    if custom_plot_func:
        fig = custom_plot_func(df)
    else:
        exp_name = f"{question_num}_{description.lower().replace(' ', '_')}"
        fig = plot_histograms(df, save_plot=False, experiment_name=exp_name)
    
    # Save plot with clear experiment naming
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"exp_{question_num}_{description.lower().replace(' ', '_')}_plot_{timestamp}.png"
    fig.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"Plot saved to {filename}")
    plt.close()
    return filename

def parse_float_number(response, min_val=0.0, max_val=0.5):
    """Parse float numbers for question 6"""
    response = response.strip()
    
    if response.startswith("ERROR:"):
        return None, "api_error"
    
    import re
    # Look for decimal numbers
    numbers = re.findall(r'\d*\.?\d+', response)
    
    if not numbers:
        return None, "no_number"
    
    try:
        number = float(numbers[0])
        if min_val <= number <= max_val:
            return number, "valid"
        else:
            return number, "out_of_range"
    except ValueError:
        return None, "parse_error"

def parse_distribution_sample(response):
    """Parse responses for question 7 (distributions)"""
    response = response.strip()
    
    if response.startswith("ERROR:"):
        return None, "api_error"
    
    import re
    # Look for any number (including negative)
    numbers = re.findall(r'-?\d*\.?\d+', response)
    
    if not numbers:
        return None, "no_number"
    
    try:
        number = float(numbers[0])
        return number, "valid"
    except ValueError:
        return None, "parse_error"

# Experiment functions for each question

def experiment_1_basic_distribution():
    """Q1: What is the distribution of numbers generated by LLMs using basic prompt?"""
    models = ["gpt-4.1", "gpt-4o", "gpt-4o-mini"]  # Mix of model sizes
    prompts = {"basic": "Guess a random number between 1 and 50"}
    
    df, filename = run_question_experiment(
        question_num=1,
        description="Basic distribution across different model sizes",
        models=models,
        prompts=prompts,
        n_samples=30
    )
    
    # Analysis and plotting
    analyze_results(df)
    calculate_randomness_metrics(df)
    plot_question_results(df, 1, "Basic Distribution")
    
    return df, filename

def experiment_2_prompt_effects():
    """Q2: How does the distribution change with different prompts?"""
    models = ["gpt-4o"]  # Single model, focus on prompt variation
    prompts = {
        "guess": "Guess a random number between 1 and 50",
        "roll_die": "Roll a 50-sided die and tell me the result",
        "random": "Generate a completely random number between 1 and 50",
        "pick": "Pick any number from 1 to 50",
        "sample": "Sample a number uniformly at random from 1 to 50"
    }
    
    df, filename = run_question_experiment(
        question_num=2,
        description="Effect of different prompts on randomness",
        models=models,
        prompts=prompts,
        n_samples=25
    )
    
    # Custom analysis for prompt comparison
    print("\n=== PROMPT COMPARISON ===")
    valid_df = df[df['validity'] == 'valid']
    for prompt in prompts.keys():
        prompt_data = valid_df[valid_df['prompt_name'] == prompt]['parsed_number']
        if len(prompt_data) > 0:
            print(f"{prompt}: mean={prompt_data.mean():.2f}, std={prompt_data.std():.2f}, unique={len(prompt_data.unique())}")
    
    # Use custom plotting function for prompt comparison
    plot_question_results(df, 2, "Prompt Effects", custom_plot_func=plot_prompt_comparison)
    return df, filename

def experiment_3_model_comparison():
    """Q3: How does the distribution change with different LLMs?"""
    models = ["gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano", "gpt-4o", "gpt-4o-mini", "o3", "o4-mini"]
    prompts = {"basic": "Guess a random number between 1 and 50"}
    
    df, filename = run_question_experiment(
        question_num=3,
        description="Model comparison across reasoning vs non-reasoning",
        models=models,
        prompts=prompts,
        n_samples=25
    )
    
    analyze_results(df)
    calculate_randomness_metrics(df)
    plot_question_results(df, 3, "Model Comparison")
    
    return df, filename

def experiment_4_temperature_effects():
    """Q4: How does the temperature setting affect the distribution?"""
    global TOKENS_USED, USE_EXISTING_DATA
    
    models = ["gpt-4o"]  # Single model, focus on temperature
    prompts = {"basic": "Guess a random number between 1 and 50"}
    temperatures = [0.0, 0.3, 0.7, 1.0, 1.5]
    
    # Check if we should load existing combined data
    if USE_EXISTING_DATA:
        combined_filename = find_latest_experiment_file(4, "temperature_effects_combined_results")
        if combined_filename:
            print(f"üìÅ Loading existing temperature effects data from: {combined_filename}")
            combined_df = pd.read_csv(combined_filename)
            
            # Print token usage (can't calculate from existing data, so skip)
            print(f"\nüìä EXPERIMENT 4 COMPLETED (USING EXISTING DATA)")
            print("Token usage not available for existing data")
            
            # Analysis
            print("\n=== TEMPERATURE EFFECTS ===")
            valid_df = combined_df[combined_df['validity'] == 'valid']
            for temp in temperatures:
                temp_data = valid_df[valid_df['temperature_label'] == temp]['parsed_number']
                if len(temp_data) > 0:
                    print(f"Temp {temp}: mean={temp_data.mean():.2f}, std={temp_data.std():.2f}, unique={len(temp_data.unique())}")
            
            plot_question_results(combined_df, 4, "Temperature Effects", custom_plot_func=plot_temperature_effects)
            return combined_df, combined_filename
    
    tokens_start = TOKENS_USED
    all_results = []
    
    # Progress bar for temperature experiments
    with tqdm(total=len(temperatures), desc="Temperature experiments") as temp_pbar:
        for temp in temperatures:
            print(f"\n--- Testing temperature {temp} ---")
            df, _ = run_question_experiment(
                question_num=f"4_temp_{temp}",
                description=f"Temperature {temp}",
                models=models,
                prompts=prompts,
                n_samples=20,
                temperature=temp
            )
            df['temperature_label'] = temp
            all_results.append(df)
            temp_pbar.update(1)
    
    # Combine all results
    combined_df = pd.concat(all_results, ignore_index=True)
    
    # Save combined results - only when generating new data
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"exp_4_temperature_effects_combined_results_{timestamp}.csv"
    combined_df.to_csv(filename, index=False)
    print(f"Combined results saved to: {filename}")
    
    # Print token usage
    tokens_used = TOKENS_USED - tokens_start
    print(f"\nüìä EXPERIMENT 4 COMPLETED")
    print(f"Tokens used for this experiment: {tokens_used:,}")
    print(f"Total tokens used so far: {TOKENS_USED:,}")
    
    # Custom temperature analysis
    print("\n=== TEMPERATURE EFFECTS ===")
    valid_df = combined_df[combined_df['validity'] == 'valid']
    for temp in temperatures:
        temp_data = valid_df[valid_df['temperature_label'] == temp]['parsed_number']
        if len(temp_data) > 0:
            print(f"Temp {temp}: mean={temp_data.mean():.2f}, std={temp_data.std():.2f}, unique={len(temp_data.unique())}")
    
    plot_question_results(combined_df, 4, "Temperature Effects", custom_plot_func=plot_temperature_effects)
    return combined_df, filename

def experiment_5_reasoning_vs_normal():
    """Q5: Do reasoning models generate more random numbers?"""
    global TOKENS_USED, USE_EXISTING_DATA
    
    reasoning_models = ["o3", "o4-mini"]
    normal_models = ["gpt-4o", "gpt-4o-mini"]
    
    prompts = {"basic": "Guess a random number between 1 and 50"}
    
    # Check if we should load existing combined data
    if USE_EXISTING_DATA:
        combined_filename = find_latest_experiment_file(5, "reasoning_vs_normal_combined_results")
        if combined_filename:
            print(f"üìÅ Loading existing reasoning vs normal data from: {combined_filename}")
            combined_df = pd.read_csv(combined_filename)
            
            print(f"\nüìä EXPERIMENT 5 COMPLETED (USING EXISTING DATA)")
            print("Token usage not available for existing data")
            
            # Analysis
            print("\n=== REASONING VS NORMAL MODELS ===")
            valid_df = combined_df[combined_df['validity'] == 'valid']
            for model_type in ['reasoning', 'normal']:
                type_data = valid_df[valid_df['model_type'] == model_type]['parsed_number']
                if len(type_data) > 0:
                    print(f"{model_type}: mean={type_data.mean():.2f}, std={type_data.std():.2f}, unique={len(type_data.unique())}")
            
            plot_question_results(combined_df, 5, "Reasoning vs Normal Models", custom_plot_func=plot_model_type_comparison)
            return combined_df, combined_filename
    
    tokens_start = TOKENS_USED
    
    # Test reasoning models
    df_reasoning, _ = run_question_experiment(
        question_num="5a",
        description="Reasoning models",
        models=reasoning_models,
        prompts=prompts,
        n_samples=25
    )
    df_reasoning['model_type'] = 'reasoning'
    
    # Test normal models  
    df_normal, _ = run_question_experiment(
        question_num="5b", 
        description="Normal models",
        models=normal_models,
        prompts=prompts,
        n_samples=25
    )
    df_normal['model_type'] = 'normal'
    
    # Combine and analyze
    combined_df = pd.concat([df_reasoning, df_normal], ignore_index=True)
    
    # Save combined results - only when generating new data
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"exp_5_reasoning_vs_normal_combined_results_{timestamp}.csv"
    combined_df.to_csv(filename, index=False)
    print(f"Combined results saved to: {filename}")
    
    # Print token usage
    tokens_used = TOKENS_USED - tokens_start
    print(f"\nüìä EXPERIMENT 5 COMPLETED")
    print(f"Tokens used for this experiment: {tokens_used:,}")
    print(f"Total tokens used so far: {TOKENS_USED:,}")
    
    print("\n=== REASONING VS NORMAL MODELS ===")
    valid_df = combined_df[combined_df['validity'] == 'valid']
    for model_type in ['reasoning', 'normal']:
        type_data = valid_df[valid_df['model_type'] == model_type]['parsed_number']
        if len(type_data) > 0:
            print(f"{model_type}: mean={type_data.mean():.2f}, std={type_data.std():.2f}, unique={len(type_data.unique())}")
    
    plot_question_results(combined_df, 5, "Reasoning vs Normal Models", custom_plot_func=plot_model_type_comparison)
    return combined_df, filename

def experiment_6_integers_vs_fractions():
    """Q6: Are integers easier or harder than fractions?"""
    global TOKENS_USED, USE_EXISTING_DATA
    
    models = ["gpt-4o"]
    
    # Check if we should load existing combined data
    if USE_EXISTING_DATA:
        combined_filename = find_latest_experiment_file(6, "integers_vs_fractions_combined_results")
        if combined_filename:
            print(f"üìÅ Loading existing integers vs fractions data from: {combined_filename}")
            combined_df = pd.read_csv(combined_filename)
            
            print(f"\nüìä EXPERIMENT 6 COMPLETED (USING EXISTING DATA)")
            print("Token usage not available for existing data")
            
            # Analysis
            print("\n=== INTEGERS VS FRACTIONS ===")
            int_valid = combined_df[(combined_df['validity'] == 'valid') & (combined_df['number_type'] == 'integer')]
            frac_valid = combined_df[(combined_df['validity'] == 'valid') & (combined_df['number_type'] == 'fraction')]
            
            int_total = len(combined_df[combined_df['number_type'] == 'integer'])
            frac_total = len(combined_df[combined_df['number_type'] == 'fraction'])
            
            print(f"Integer validity: {len(int_valid)}/{int_total} ({len(int_valid)/int_total*100:.1f}%)")
            print(f"Fraction validity: {len(frac_valid)}/{frac_total} ({len(frac_valid)/frac_total*100:.1f}%)")
            
            if len(int_valid) > 0:
                print(f"Integer unique values: {len(int_valid['parsed_number'].unique())}")
            if len(frac_valid) > 0:
                print(f"Fraction unique values: {len(frac_valid['parsed_number'].unique())}")
            
            plot_question_results(combined_df, 6, "Integers vs Fractions", custom_plot_func=plot_number_type_comparison)
            return combined_df, combined_filename
    
    tokens_start = TOKENS_USED
    
    # Integer prompts
    int_prompts = {"integer": "Guess a random number between 1 and 50"}
    df_int, _ = run_question_experiment(
        question_num="6a",
        description="Integer sampling",
        models=models,
        prompts=int_prompts,
        n_samples=25
    )
    
    # Fraction prompts
    frac_prompts = {"fraction": "Generate a random decimal number between 0.0 and 0.5"}
    df_frac, _ = run_question_experiment(
        question_num="6b",
        description="Fraction sampling", 
        models=models,
        prompts=frac_prompts,
        n_samples=25,
        custom_parser=parse_float_number
    )
    
    # Print token usage
    tokens_used = TOKENS_USED - tokens_start
    print(f"\nüìä EXPERIMENT 6 COMPLETED")
    print(f"Tokens used for this experiment: {tokens_used:,}")
    print(f"Total tokens used so far: {TOKENS_USED:,}")
    
    # Analysis
    print("\n=== INTEGERS VS FRACTIONS ===")
    int_valid = df_int[df_int['validity'] == 'valid']
    frac_valid = df_frac[df_frac['validity'] == 'valid']
    
    print(f"Integer validity: {len(int_valid)}/{len(df_int)} ({len(int_valid)/len(df_int)*100:.1f}%)")
    print(f"Fraction validity: {len(frac_valid)}/{len(df_frac)} ({len(frac_valid)/len(df_frac)*100:.1f}%)")
    
    if len(int_valid) > 0:
        print(f"Integer unique values: {len(int_valid['parsed_number'].unique())}")
    if len(frac_valid) > 0:
        print(f"Fraction unique values: {len(frac_valid['parsed_number'].unique())}")
    
    # Save combined - only when generating new data
    df_int['number_type'] = 'integer'
    df_frac['number_type'] = 'fraction'
    combined_df = pd.concat([df_int, df_frac], ignore_index=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"exp_6_integers_vs_fractions_combined_results_{timestamp}.csv"
    combined_df.to_csv(filename, index=False)
    print(f"Combined results saved to: {filename}")
    
    plot_question_results(combined_df, 6, "Integers vs Fractions", custom_plot_func=plot_number_type_comparison)
    return combined_df, filename

def experiment_7_distributions():
    """Q7: Is it harder to sample from non-uniform distributions?"""
    models = ["gpt-4o"]
    prompts = {
        "uniform": "Generate a random number between 1 and 50",
        "normal": "Generate a random number from a standard normal distribution (mean=0, std=1)",
        "exponential": "Generate a random number from an exponential distribution with rate=1"
    }
    
    df, filename = run_question_experiment(
        question_num=7,
        description="Non-uniform distributions",
        models=models,
        prompts=prompts,
        n_samples=30,
        custom_parser=parse_distribution_sample
    )
    
    # Analysis by distribution type
    print("\n=== DISTRIBUTION SAMPLING ===")
    valid_df = df[df['validity'] == 'valid']
    for prompt in prompts.keys():
        prompt_data = valid_df[valid_df['prompt_name'] == prompt]['parsed_number']
        if len(prompt_data) > 0:
            print(f"{prompt}: mean={prompt_data.mean():.2f}, std={prompt_data.std():.2f}, unique={len(prompt_data.unique())}")
            print(f"  Range: {prompt_data.min():.2f} to {prompt_data.max():.2f}")
    
    plot_question_results(df, 7, "Distribution Sampling", custom_plot_func=plot_distribution_comparison)
    return df, filename

def experiment_8_different_ranges():
    """Q8: Do LLMs have an easier time sampling from different ranges?"""
    global TOKENS_USED, USE_EXISTING_DATA
    
    models = ["gpt-4o"]
    prompts = {
        "small": "Guess a random number between 1 and 10", 
        "medium": "Guess a random number between 1 and 50",
        "large": "Guess a random number between 1000 and 2000",
        "huge": "Guess a random number between 100000 and 1000000"
    }
    
    # Check if we should load existing data
    if USE_EXISTING_DATA:
        filename = find_latest_experiment_file(8)
        if filename:
            print(f"üìÅ Loading existing range effects data from: {filename}")
            df = pd.read_csv(filename)
            
            print(f"\nüìä EXPERIMENT 8 COMPLETED (USING EXISTING DATA)")
            print("Token usage not available for existing data")
            
            # Analysis by range
            print("\n=== RANGE EFFECTS ===")
            valid_df = df[df['validity'] == 'valid']
            for range_type in prompts.keys():
                range_data = valid_df[valid_df['range_type'] == range_type]
                if len(range_data) > 0:
                    numbers = range_data['parsed_number']
                    print(f"{range_type}: validity={len(range_data)}/{25}, unique={len(numbers.unique())}")
                    if len(numbers) > 0:
                        print(f"  Mean: {numbers.mean():.0f}, Std: {numbers.std():.2f}")
            
            plot_question_results(df, 8, "Different Ranges", custom_plot_func=plot_range_comparison)
            return df, filename
    
    tokens_start = TOKENS_USED
    
    # Custom parser for different ranges
    def parse_range_number(response, prompt_name):
        ranges = {
            "small": (1, 10),
            "medium": (1, 50), 
            "large": (1000, 2000),
            "huge": (100000, 1000000)
        }
        min_val, max_val = ranges[prompt_name]
        return parse_number(response, min_val, max_val)
    
    results = []
    total_iterations = len(models) * len(prompts) * 25
    
    with tqdm(total=total_iterations, desc="Experiment 8 - Range Effects") as pbar:
        for model in models:
            for prompt_name, prompt_text in prompts.items():
                print(f"\nTesting {model} with range '{prompt_name}'...")
                
                for i in range(25):
                    response = get_random_number(model, prompt_text, 0.7)
                    number, validity = parse_range_number(response, prompt_name)
                    
                    results.append({
                        'question': 8,
                        'timestamp': datetime.now().isoformat(),
                        'model': model,
                        'prompt_name': prompt_name,
                        'prompt_text': prompt_text,
                        'temperature': 0.7,
                        'raw_response': response,
                        'parsed_number': number,
                        'validity': validity,
                        'sample_id': i,
                        'range_type': prompt_name
                    })
                    
                    pbar.update(1)
                    time.sleep(0.1)
    
    df = pd.DataFrame(results)
    
    # Save results - only when generating new data
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"exp_8_different_ranges_results_{timestamp}.csv"
    df.to_csv(filename, index=False)
    print(f"Results saved to: {filename}")
    
    # Print token usage
    tokens_used = TOKENS_USED - tokens_start
    print(f"\nüìä EXPERIMENT 8 COMPLETED")
    print(f"Tokens used for this experiment: {tokens_used:,}")
    print(f"Total tokens used so far: {TOKENS_USED:,}")
    
    # Analysis by range
    print("\n=== RANGE EFFECTS ===")
    valid_df = df[df['validity'] == 'valid']
    for range_type in prompts.keys():
        range_data = valid_df[valid_df['range_type'] == range_type]
        if len(range_data) > 0:
            numbers = range_data['parsed_number']
            print(f"{range_type}: validity={len(range_data)}/{25}, unique={len(numbers.unique())}")
            if len(numbers) > 0:
                print(f"  Mean: {numbers.mean():.0f}, Std: {numbers.std():.2f}")
    
    plot_question_results(df, 8, "Different Ranges", custom_plot_func=plot_range_comparison)
    return df, filename

# Master function to run all experiments
def run_all_experiments():
    """Run all 8 experiments systematically"""
    global TOKENS_USED
    
    print("üöÄ STARTING COMPREHENSIVE LLM RANDOMNESS STUDY")
    print("=" * 80)
    
    initial_tokens = TOKENS_USED
    
    experiments = [
        experiment_1_basic_distribution,
        experiment_2_prompt_effects, 
        experiment_3_model_comparison,
        experiment_4_temperature_effects,
        experiment_5_reasoning_vs_normal,
        experiment_6_integers_vs_fractions,
        experiment_7_distributions,
        experiment_8_different_ranges
    ]
    
    results = {}
    
    # Progress bar for all experiments
    with tqdm(total=len(experiments), desc="Overall Progress") as overall_pbar:
        for i, experiment_func in enumerate(experiments, 1):
            try:
                print(f"\nüî¨ Running Experiment {i}...")
                df, filename = experiment_func()
                results[i] = (df, filename)
                print(f"‚úÖ Experiment {i} completed successfully!")
                
                # Save progress
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                with open(f"exp_progress_{timestamp}.txt", "w") as f:
                    f.write(f"Completed experiments: {list(results.keys())}\n")
                    f.write(f"Total tokens used: {TOKENS_USED:,}\n")
                
                overall_pbar.update(1)
                    
            except Exception as e:
                print(f"‚ùå Experiment {i} failed: {str(e)}")
                overall_pbar.update(1)
                continue
    
    total_tokens_used = TOKENS_USED - initial_tokens
    print(f"\nüéâ STUDY COMPLETE!")
    print(f"Total experiments completed: {len(results)}/8")
    print(f"Total tokens used for all experiments: {total_tokens_used:,}")
    print(f"Grand total tokens used: {TOKENS_USED:,}")
    
    return results

def set_data_mode(use_existing=False, data_dir="."):
    """Set whether to use existing data or generate new data"""
    global USE_EXISTING_DATA, DATA_DIRECTORY
    USE_EXISTING_DATA = use_existing
    DATA_DIRECTORY = data_dir
    
    if use_existing:
        print("üîÑ MODE: Using existing data files")
        print(f"üìÇ Data directory: {data_dir}")
    else:
        print("üÜï MODE: Generating new data")

def run_analysis_only():
    """Run only analysis and plotting on existing data"""
    global USE_EXISTING_DATA
    
    print("üìä ANALYSIS-ONLY MODE: Loading existing data and generating plots")
    print("=" * 70)
    
    old_mode = USE_EXISTING_DATA
    USE_EXISTING_DATA = True
    
    try:
        # Run all experiments in analysis-only mode
        results = run_all_experiments()
        return results
    finally:
        USE_EXISTING_DATA = old_mode

def list_available_data():
    """List all available experiment data files"""
    print("üìã AVAILABLE EXPERIMENT DATA FILES:")
    print("=" * 50)
    
    for exp_num in range(1, 9):
        files = glob.glob(f"exp_{exp_num}_*.csv")
        if files:
            files.sort(key=os.path.getmtime, reverse=True)
            print(f"Experiment {exp_num}:")
            for file in files:
                mtime = datetime.fromtimestamp(os.path.getmtime(file))
                size_kb = os.path.getsize(file) / 1024
                print(f"  üìÑ {file} ({size_kb:.1f} KB, {mtime.strftime('%Y-%m-%d %H:%M')})")
        else:
            print(f"Experiment {exp_num}: No data files found")
    
    print()

# MAIN EXECUTION
if __name__ == "__main__":
    print("üß™ LLM RANDOMNESS RESEARCH SUITE")
    print("=" * 50)
    
    # Check command line arguments for mode selection
    import sys
    if len(sys.argv) > 1:
        if sys.argv[1] == "--analysis-only":
            set_data_mode(use_existing=True)
            run_analysis_only()
        elif sys.argv[1] == "--list-data":
            list_available_data()
        elif sys.argv[1] == "--help":
            print("Usage:")
            print("  python llm-rng.py                 # Generate new data and run all experiments")
            print("  python llm-rng.py --analysis-only # Load existing data and run analysis/plotting only")
            print("  python llm-rng.py --list-data     # List available data files")
            print("  python llm-rng.py --help          # Show this help")
        else:
            print(f"Unknown argument: {sys.argv[1]}")
            print("Use --help for usage information")
    else:
        print("Running ALL 8 experiments to answer research questions")
        print("Set USE_EXISTING_DATA = True at the top of the file to use existing data")
        print("=" * 50)
        
        # Run all experiments
        run_all_experiments()